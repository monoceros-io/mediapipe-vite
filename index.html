<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MediaPipe Image Segmenter with Pose Landmarker</title>
    <style>
        body {
            padding: 0;
            margin: 0;
        }

        .d-canv {
            border: 1px solid red;
        }

        video {
            position: absolute;
            top: 0;
        }

        canvas {
            position: absolute;
            top: 0;
            /* opacity: 0.5; */
        }
    </style>
</head>

<body>
    <video id="webcam" autoplay playsinline></video>
    <canvas class="d-canv" id="dump-canvas-0" width="640" height="480"></canvas>

    <script type="module">
        import { ImageSegmenter, FilesetResolver } from "./node_modules/@mediapipe/tasks-vision";
        import { PoseLandmarker, DrawingUtils } from "https://cdn.skypack.dev/@mediapipe/tasks-vision@0.10.0";

        let segmenter, poseLandmarker;
        const video = document.getElementById("webcam");
        const dumpCanvas = document.getElementById("dump-canvas-0");
        const dumpCtx = dumpCanvas.getContext("2d");
        const runningMode = "VIDEO";
        let frameCounter = 0; // Counter to track frames

        async function loadModels() {
            const vision = await FilesetResolver.forVisionTasks("./node_modules/@mediapipe/tasks-vision/wasm");

            segmenter = await ImageSegmenter.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: "./selfie_multiclass_256x256.tflite",
                    delegate: 'GPU'
                },
                runningMode: runningMode,
            });

            poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task`,
                    delegate: "GPU"
                },
                runningMode: runningMode,
                numPoses: 1
            });

            console.log("Models loaded successfully.");
        }

        async function startWebcam() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.addEventListener("loadeddata", processVideoFrame);
            } catch (error) {
                console.error("Error accessing webcam:", error);
            }
        }

        async function processVideoFrame() {
            if (!segmenter || !poseLandmarker) {
                console.error("Models not loaded.");
                return;
            }

            const offscreenCanvas = document.createElement("canvas");
            const offscreenCtx = offscreenCanvas.getContext("2d");
            offscreenCanvas.width = video.videoWidth;
            offscreenCanvas.height = video.videoHeight;
            offscreenCtx.drawImage(video, 0, 0, offscreenCanvas.width, offscreenCanvas.height);

            const imageData = offscreenCtx.getImageData(0, 0, offscreenCanvas.width, offscreenCanvas.height);

            // Perform segmentation
            const segmentationResult = await segmenter.segmentForVideo(imageData, performance.now());
            if (segmentationResult?.confidenceMasks) {
                drawAllMasksToDumpCanvas(segmentationResult.confidenceMasks);
                segmentationResult.confidenceMasks.forEach(mask => mask.close());
            }

            // Perform pose landmark detection every 5 frames
            // if (frameCounter % 5 === 0) {
            poseLandmarker.detectForVideo(video, performance.now(), drawPoseLandmarks);
            // }

            frameCounter++;
            requestAnimationFrame(processVideoFrame);
        }

        function drawAllMasksToDumpCanvas(confidenceMasks) {
            const { width, height } = confidenceMasks[0];
            if (dumpCanvas.width !== width || dumpCanvas.height !== height) {
                dumpCanvas.width = width;
                dumpCanvas.height = height;
            }

            const imageData = dumpCtx.createImageData(width, height);
            const data = imageData.data;

            const colors = [
                [255, 0, 0],   // Red BACKGROUND
                [0, 255, 0],   // Green HAIR
                [0, 0, 255],   // Blue SKIN
                [255, 255, 0], // Yellow FACE
                [0, 255, 255], // Cyan CLOTHES
                [255, 0, 255]  // Magenta OBJECTS
            ];

            // Only draw masks 0 and 4
            [0, 4].forEach(maskIndex => {
                const maskData = confidenceMasks[maskIndex];
                const maskArray = maskData.getAsFloat32Array();
                const [r, g, b] = colors[maskIndex % colors.length];

                for (let i = 0; i < maskArray.length; i++) {
                    const alpha = maskArray[i] * 255;
                    const offset = i * 4;

                    data[offset] = Math.max(data[offset], r * (alpha / 255));
                    data[offset + 1] = Math.max(data[offset + 1], g * (alpha / 255));
                    data[offset + 2] = Math.max(data[offset + 2], b * (alpha / 255));
                    data[offset + 3] = 255;
                }
            });

            dumpCtx.putImageData(imageData, 0, 0);
        }

        function drawPoseLandmarks(poseResult) {
            const drawingUtils = new DrawingUtils(dumpCtx);

            for (const landmark of poseResult.landmarks) {
                drawingUtils.drawLandmarks(landmark, {
                    radius: data => DrawingUtils.lerp(data.from.z, -0.15, 0.1, 5, 1)
                });
                drawingUtils.drawConnectors(landmark, PoseLandmarker.POSE_CONNECTIONS);
            }
        }

        async function processMasksInWASM(confidenceMasks, colors, data) {
            // Ensure the arrays are of correct size before processing
            const maskIndices = new Int32Array([0, 1, 4]);
            const masks = confidenceMasks.map(m => new Float32Array(m.getAsFloat32Array()));
            const wasmColors = new Uint8Array(colors.flat());
            const wasmData = new Uint8Array(data);

            console.log("Masks size:", masks.length);
            console.log("Colors size:", wasmColors.length);
            console.log("Data size:", wasmData.length);

            // Ensure that the arrays fit into the allocated WASM memory
            if (wasmData.byteLength > wasmInstance.memory.buffer.byteLength) {
                console.warn("WASM memory size is not large enough to handle the data.");
                return;
            }

            // Call the WASM function
            wasmInstance.processMasks(maskIndices, masks, wasmColors, wasmData);

            return wasmData;
        }

        loadModels();
        startWebcam();

        let wasmInstance = null;

        WebAssembly.instantiateStreaming(fetch("index.wasm"), {
            env: {
                memory: new WebAssembly.Memory({ initial: 512, maximum: 1024 }), // Increased memory
                abort: () => console.error("WASM aborted"),
            }
        }).then(result => {
            wasmInstance = result.instance;
            
        }).catch(err => {
            console.error("WASM failed to initialize", err);
        });

    </script>
</body>

</html>
